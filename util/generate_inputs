#!/usr/bin/env python3

from datetime import datetime
from argparse import ArgumentParser
import pandas as pd
import re
import json
from ast import literal_eval


today = datetime.today()
date = today.strftime("%Y-%m-%d")

def main(args):
    run_project_cohort_analysis = args.run_project_cohort_analysis
    cohort_staging_bucket_type = "uat"

    projects = dict()
    for project_tsv in args.project_tsvs:
        project_df = pd.read_csv(project_tsv, delimiter="\t")
        # Currently all samples for a team share an embargo status
        project_embargo_status = list(set(project_df.embargoed))
        if len(project_embargo_status) > 1:
            raise SystemExit(
                f"More than one embargo status found for samples in {project_tsv}; don't know how to handle this."
            )
        else:
            staging_bucket_types = (
                ["dev"] if project_embargo_status[0] is True else ["dev", "uat"]
            )
            # If any of the teams' datasets are embargoed, we'll set the cohort bucket to embargoed also
            if project_embargo_status[0] is True:
                cohort_staging_bucket_type = "dev"
        for index, row in project_df.iterrows():
            project_id = row.project_id
            sample_id = row.ASAP_sample_id
            batch = str(row.batch)
            fastq_R1s = literal_eval(row.fastq_R1s)
            fastq_R2s = literal_eval(row.fastq_R2s)
            fastq_I1s = literal_eval(row.fastq_I1s)
            fastq_I2s = literal_eval(row.fastq_I2s)

            sample = {
                "sample_id": sample_id,
                "fastq_R1s": fastq_R1s,
                "fastq_R2s": fastq_R2s,
                "fastq_I1s": fastq_I1s,
                "fastq_I2s": fastq_I2s,
            }

            if not pd.isna(batch):
                sample["batch"] = batch

            if project_id not in projects:
                staging_data_buckets = [
                    f"gs://asap-{staging_bucket_type}-{project_id}-{args.source}-{args.team_dataset}"
                    for staging_bucket_type in staging_bucket_types
                ]

                projects[project_id] = {
                    "project_id": project_id,
                    "samples": [sample],
                    "run_project_cohort_analysis": run_project_cohort_analysis,
                    "raw_data_bucket": f"gs://asap-raw-{project_id}-{args.source}-{args.team_dataset}",
                    "staging_data_buckets": staging_data_buckets,
                }

                if args.workflow_name == "pmdbs_bulk_rnaseq":
                    projects[project_id]["project_sample_metadata_csv"] = f"gs://asap-raw-{project_id}-{args.source}-{args.team_dataset}/metadata/SAMPLE.csv"
            else:
                projects[project_id]["samples"].append(sample)

    projects = [project for project in projects.values()]

    with open(args.inputs_template, "r") as f:
        inputs_json = json.load(f)

    projects_key = [
        key for key in inputs_json.keys() if re.search(r"^[^\.]*\.projects$", key)
    ]

    if len(projects_key) != 1:
        raise SystemExit(
            f"Failed to find projects key in the inputs template\nProjects keys found: {projects_key}\nAvailable keys: {inputs_json.keys()}"
        )
    else:
        projects_key = projects_key[0]

        inputs_json[projects_key] = projects

    cohort_staging_data_buckets = [
        f"gs://asap-{cohort_staging_bucket_type}-cohort-{args.source}-{args.cohort_dataset}"
    ]
    inputs_json[
        f"{args.workflow_name}.cohort_staging_data_buckets"
    ] = cohort_staging_data_buckets

    if args.output_file_prefix is None:
        output_file = f"inputs.{cohort_staging_bucket_type}.{args.source}-{args.cohort_dataset}.{date}.json"
    else:
        output_file = f"{args.output_file_prefix}.{cohort_staging_bucket_type}.json"
    with open(output_file, "w") as f:
        json.dump(inputs_json, f, indent=4)
    print(f"Wrote input JSON file: {output_file}")


if __name__ == "__main__":
    parser = ArgumentParser(
        description="Given a TSV of sample information, generate an inputs JSON"
    )

    parser.add_argument(
        "-p",
        "--project-tsv",
        dest="project_tsvs",
        type=str,
        action="append",
        required=True,
        help="Project TSV including information for samples present in the project; columns project_id, sample_id, batch, fastq_path. Can provide one per project, or include all samples in a single TSV.",
    )
    parser.add_argument(
        "-i",
        "--inputs-template",
        type=str,
        required=True,
        help="Template JSON file to add project information to (projects will be added at the *.projects key).",
    )
    parser.add_argument(
        "-c",
        "--run-project-cohort-analysis",
        action="store_true",
        required=False,
        help="Run project-level cohort analysis. This will be set for all projects included in the cohort [false].",
    )
    parser.add_argument(
        "-w",
        "--workflow-name",
        type=str,
        required=True,
        help="WDL workflow name.",
    )
    parser.add_argument(
        "-s",
        "--source",
        type=str,
        required=True,
        help="Source name in bucket name.",
    )
    parser.add_argument(
        "-d",
        "--team-dataset",
        type=str,
        required=True,
        help="Dataset name in team bucket name.",
    )
    parser.add_argument(
        "-b",
        "--cohort-dataset",
        type=str,
        required=True,
        help="Dataset name in cohort bucket name.",
    )
    parser.add_argument(
        "-o",
        "--output-file-prefix",
        required=False,
        default=None,
        help="Prefix for output JSON file to write workflow inputs to [inputs.{cohort_staging_bucket_type}.{source}-{cohort_dataset}.{date}.json].",
    )

    args = parser.parse_args()

    main(args)
