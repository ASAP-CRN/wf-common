#!/usr/bin/env python3

# Transfer locally saved QC'd metadata to the raw Google bucket
# Usage: 
#   Single dataset:
#     python3 transfer_qc_metadata_to_raw_bucket -t jakobsson -ds pmdbs-bulk-rnaseq
#
# Batch processing (datasets.csv field 1 is team, field 2 is dataset, no header):
#     while IFS=, read -r team dataset; do
#         python3 transfer_qc_metadata_to_raw_bucket -t "$team" -ds "$dataset" -p
#     done < datasets.csv
#
# Defaults to dry run unless -p flag is added!
# 
# NOTE on assumptions made:
# 1.   You have cloned asap-crn-cloud-dataset-metadata and its root is at the
# ---- same level as wf-common
# 2.   The metadata/ directory of the target dataset exists locally anc contains
# ---- the finalized QC'd metadata structure (original/, cde/, and release/ dirs)
# 3.   The file_metadata/ directory exists locally and is populated with the files
# ---- artifacts.csv, raw_files.csv, and curated_files.csv

import argparse
import logging
from pathlib import Path
from common import gsync, check_bucket_exists, strip_team_name, gcopy


logging.basicConfig(
	level=logging.INFO,
	format="%(asctime)s - %(levelname)s - %(message)s"
)

# NOTE: Assumes that you have cloned asap-crn-cloud-dataset-metadata, which
# contains the dataset directories locally and the code to QC metadata.
repo_root = Path(__file__).resolve().parents[1]
metadata_root = repo_root.parent / "asap-crn-cloud-dataset-metadata"

FILE_METADATA_FILES_TO_SYNC = [
    "artifacts.csv",
    "raw_files.csv",
    "curated_files.csv",
]


def check_orig_metadata_exists(orig_dir: Path) -> bool:
    """Check that original metadata directory exists locally"""
    orig_dir = Path(orig_dir)
    if not orig_dir.exists():
        return False
    
    # Check for any CSV files (exact tables may not be consistent at this stage)
    csv_files = list(orig_dir.glob("*.csv"))
    return len(csv_files) > 0


def main(args):

    dry_run = not args.promote
    
    team_name = strip_team_name(args.team_name)
    dataset_name = args.dataset_name
    dataset_name_long = f"{team_name}-{dataset_name}"
    
    dataset_dir = metadata_root / "datasets" / dataset_name_long
    metadata_dir = dataset_dir / "metadata"
    orig_dir = metadata_dir / "original"
    file_metadata_dir = dataset_dir / "file_metadata"
    
    # No team- prefix for cohorts
    if team_name == "cohort":
        bucket_name = f"gs://asap-raw-{team_name}-{dataset_name}"
    else:
        bucket_name = f"gs://asap-raw-team-{team_name}-{dataset_name}"
    
    file_metadata_bucket = f"{bucket_name}/file_metadata"
    metadata_bucket = f"{bucket_name}/metadata"
    
    check_bucket_exists(bucket_name)

    
    # Transers entire metadata/ directory
    if metadata_dir.exists():
        logging.info(f"Transferring local metadata directory to [{metadata_bucket}]")
        
        # Early exit if original metadata is missing: implies incomplete QC
        if not check_orig_metadata_exists(orig_dir):
            raise ValueError(f"Original metadata directory is missing or empty: {orig_dir}")
        
        gsync(source_path=str(metadata_dir), 
              destination_path=metadata_bucket, 
              dry_run=dry_run)
    else:
        raise ValueError(f"Local metadata directory not found: {metadata_dir}")

    # Transfer file_metadata/ files
    if file_metadata_dir.exists():
        logging.info(f"Transferring selected local file_metadata directory to [{file_metadata_bucket}]")
        for file_name in FILE_METADATA_FILES_TO_SYNC:
            file_path = file_metadata_dir / file_name
            if file_path.exists():
                if dry_run:
                    logging.info(f"Would copy {file_path} to {file_metadata_bucket}/{file_name}")
                else:
                    logging.info(f"Transferring file: {file_name}")
                    gcopy(source_path=str(file_path), 
                          destination_path=f"{file_metadata_bucket}/{file_name}",
                          recursive=False)
            else:
                logging.warning(f"File not found, skipping: {file_name}")
    else:
        raise ValueError(f"Local file metadata directory not found: {file_metadata_dir}")
    
    
    

if __name__ == "__main__":
    
    parser = argparse.ArgumentParser(
		description="Sync local metadata directories to raw bucket"
	)
    
    parser.add_argument(
        "-t",
        "--team_name",
        required=True,
        help="The team name of the dataset"
    )
    parser.add_argument(
        "-ds",
        "--dataset_name",
        required=True,
        help="The name of the dataset"
    )
    parser.add_argument(
        "-p",
		"--promote",
		action="store_true",
		required=False,
		help="Promote data (default is dry run)."
	)

    args = parser.parse_args()
    main(args)
