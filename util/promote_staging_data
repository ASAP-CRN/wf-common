#!/usr/bin/env python3

import argparse
import sys
import logging
from datetime import datetime
from google.cloud import storage
from common import (
	list_teams,
	list_gs_files,
	read_manifest_files,
	md5_check,
	non_empty_check,
	associated_metadata_check,
	gsync,
)

ALL_TEAMS = [
	"cohort",
	"team-hafler",
	"team-hardy",
	"team-jakobsson",
	"team-lee",
	"team-scherzer",
	"team-sulzer",
	"team-voet",
	"team-wood"
]

GREEN_CHECKMARK = "✅"
RED_X = "❌"

logging.basicConfig(
	level=logging.INFO,
	format="%(asctime)s - %(levelname)s - %(message)s",
	handlers=[
		logging.FileHandler("promote_staging_data_script.log"),
		logging.StreamHandler()
	]
)

def main(args):
	if args.list:
		list_teams()
		sys.exit(0)

	for team in args.teams:
		if team not in ALL_TEAMS:
			logging.error(f"Team [{team}] is not one of the available teams")
			list_teams()
			sys.exit(1)

	dry_run = not args.promote
	namespaces = [args.staging, "curated"]
	client = storage.Client()

	for team, dataset in zip(args.teams, args.dataset):
		for env in namespaces:
			bucket_name = f"asap-{env}-{team}-{args.source}-{dataset}"
			bucket = client.get_bucket(bucket_name)
			results = {}

			# Data integrity tests
			logging.info(f"Running data integrity tests on [{bucket_name}]")
			blob_names, gs_files, sample_list_loc = list_gs_files(bucket, args.workflow_name)
			combined_manifest_df = read_manifest_files(bucket, args.workflow_name)
			md5_hashes = md5_check(bucket, args.workflow_name)
			not_empty_test_results = non_empty_check(bucket, args.workflow_name, GREEN_CHECKMARK, RED_X)
			metadata_present_test_results = associated_metadata_check(combined_manifest_df, gs_files, GREEN_CHECKMARK, RED_X)
			results[env] = {
				"blob_names": blob_names,
				"gs_files": gs_files,
				"sample_list_loc": sample_list_loc,
				"combined_manifest_df": combined_manifest_df,
				"md5_hashes": md5_hashes,
				"not_empty_test_results": not_empty_test_results,
				"metadata_present_test_results": metadata_present_test_results,
			}

		# Compare different envs
		same_files, new_files, deleted_files = compare_blob_names(results, args.staging)
		modified_files = compare_md5_hashes(results, args.staging, same_files)


			# Generate report

			# TODO - if any test result failed, do not promote
			# Try syncing staging data to production
			staging_bucket = f"gs://{bucket_name}"
			production_bucket = f"gs://asap-curated-{team}-{args.source}-{dataset}"

			logging.info(f"Promoting [{team}] data to production")
			logging.info(f"\tStaging bucket:\t\t[{staging_bucket}]")
			logging.info(f"\tProduction bucket:\t[{production_bucket}]")

			gsync(staging_bucket, production_bucket, dry_run)


if __name__ == "__main__":
	parser = argparse.ArgumentParser(
		description="Promote data in staging buckets to production."
	)

	parser.add_argument(
		"-t",
		"--teams",
		type=str,
		nargs='+',
		required=True,
		help="Space-delimited team(s) to promote data for."
	)
	parser.add_argument(
		"-l",
		"--list",
		action="store_true",
		required=False,
		help="List available teams."
	)
	parser.add_argument(
		"-s",
		"--source",
		type=str,
		required=True,
		help="Source name in bucket name."
	)
	parser.add_argument(
		"-d",
		"--dataset",
		type=str,
		nargs='+',
		required=True,
		help="Space-delimited dataset name(s) in team bucket name, must follow the same order as {team}."
	)
	parser.add_argument(
		"-w",
		"--workflow-name",
		type=str,
		required=True,
		help="Workflow name used as a directory in bucket."
	)
	parser.add_argument(
		"-p",
		"--promote",
		action="store_true",
		required=False,
		help="Promote data (default is dry run)."
	)
	parser.add_argument(
		"-s",
		"--staging",
		choices=["uat", "dev"],
		default="uat",
		required=False,
		help="Staging bucket type (uat or dev) [uat]."
	)

	args = parser.parse_args()

	if len(args.team) != len(args.dataset):
        parser.error("The number of teams and datasets must be the same.")

	main(args)
